# Dynamic Benchmark Suite for `at`

This is the fully dynamic benchmark suite for the `at` programming language. Instead of hardcoding prompts and tests, it automatically streams the **AutoCodeBenchmark** JSONL directly from HuggingFace, ensuring you are testing against a massive, recognized multi-lingual academic standard.

## How it Workflow

1. **Download**: `run_autocodebench.py` downloads a subset of the dataset from HuggingFace (`autocodebench_lite.jsonl`).
2. **Contextualization**: It extracts the `canonical_solution` (often C++ or Python) to compute the baseline standard token usage.
3. **LLM Translation & Generation**: It passes the Problem description and the original language test arrays (`full_test_func`) to the LLM along with `skill.md` (the core of `at`'s procedural standard). It asks the LLM to write the solution and transpile the tests into native `at` `test { ... }` blocks.
4. **Execution**: It compiles and runs `at test` on the generated code.
5. **Score**: It outputs Pass@1 rate, Execution time, and crucially, the **Token Savings** percentage generated by writing logic in `at` versus the original language!

## Getting Started

```bash
cd benchmarks
pip install openai tiktoken
```

Set your API variables (defaults to local Ollama API for models like `llama3.2` or `qwen2.5-coder`):

```bash
export OPENAI_BASE_URL="https://api.openai.com/v1"
export OPENAI_API_KEY="sk-..."
```

## Running the Benchmark

You can evaluate any LLMs. By default, it takes the first 3 problems and evaluates.

```bash
python3 run_autocodebench.py --models "gpt-4o,claude-3-5-sonnet" --count 10
```

*Note: You may see `‚ùå FAIL!` if an LLM hallucinates `at` syntax for highly complex multi-dimensional algorithmic tasks absent from its training data, but as foundation models improve, and utilizing the RAG capabilities with `skill.md`, Pass@1 scores will continually increase.*
