# Dynamic Benchmark Suite for `at`

This is the fully dynamic benchmark suite for the `at` programming language. Instead of hardcoding prompts and tests, it automatically streams the **AutoCodeBenchmark** JSONL directly from HuggingFace, ensuring you are testing against a massive, recognized multi-lingual academic standard.

## How it Workflow

1. **Download**: `run_autocodebench.py` downloads a subset of the dataset from HuggingFace (`autocodebench_lite.jsonl`).
2. **Contextualization**: It extracts the `canonical_solution` (often C++ or Python) to compute the baseline standard token usage.
3. **LLM Translation & Generation**: It passes the Problem description and the original language test arrays (`full_test_func`) to the LLM along with `skill.md` (the core of `at`'s procedural standard). It asks the LLM to write the solution and transpile the tests into native `at` `test { ... }` blocks.
4. **Execution**: It compiles and runs `at test` on the generated code.
5. **Score**: It outputs Pass@1 rate, Execution time, and crucially, the **Token Savings** percentage generated by writing logic in `at` versus the original language!

## Getting2. **Set your API Keys**

   The harness uses the standard OpenAI python SDK, meaning it can connect to OpenAI, Anthropic (via wrappers), Groq, OpenRouter, or **z.ai** (Zhipu AI).

   *Example: Z.ai (Zhipu AI) for GLM-4.5*

   ```bash
   export OPENAI_BASE_URL="https://open.bigmodel.cn/api/paas/v4/"
   export OPENAI_API_KEY="your-zhipu-api-key-here"
   ```

   *Example: Local Ollama*

   ```bash
   export OPENAI_BASE_URL="http://localhost:11434/v1"
   export OPENAI_API_KEY="ollama"
   ```

   *Example: OpenAI*

   ```bash
   export OPENAI_BASE_URL="https://api.openai.com/v1"
   export OPENAI_API_KEY="sk-..."
   ```

1. **Execute the Benchmark**
   Run the evaluation script. You can specify `--models` (like `glm-4.5`) and the `--count` of problems to evaluate.

   ```bash
   python3 run_autocodebench.py --models "glm-4.5" --count 10
   ```

*Note: You may see `‚ùå FAIL!` if an LLM hallucinates `at` syntax for highly complex multi-dimensional algorithmic tasks absent from its training data, but as foundation models improve, and utilizing the RAG capabilities with `skill.md`, Pass@1 scores will continually increase.*
